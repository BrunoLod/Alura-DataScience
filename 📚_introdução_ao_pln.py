# -*- coding: utf-8 -*-
"""üìö Introdu√ß√£o ao PLN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Kxt3Igpvz7AUmomgAQFMykb6fgEVG_Y

### üìö Introdu√ß√£o ao Processamento de Linguagem Natural (PLN):

Ol√°! Bem-vindas e bem vindos ao fascinante mundo do Processamento de Linguagem Natural (PLN)! Nesta jornada, irei explorar os passos necess√°rios que fornecem aos computadores a capacidade de entender e processar a linguagem humana.

üó£Ô∏è A linguagem humana √© incrivelmente complexa e rica em nuances. Compreender e interpretar corretamente textos, conversas e at√© mesmo o tom de voz √© uma habilidade natural para n√≥s, seres humanos. No entanto, para os computadores, essa tarefa √© muito mais desafiadora.

ü§ñ O PLN √© um campo da intelig√™ncia artificial (IA) que se concentra em desenvolver algoritmos e t√©cnicas para permitir que os computadores compreendam, analisem e gerem texto de maneira semelhante aos humanos. Ele abrange uma ampla gama de aplica√ß√µes, desde assistentes virtuais e tradutores autom√°ticos at√© sistemas de recomenda√ß√£o e an√°lise de sentimentos.

üí° Com o PLN, podemos automatizar tarefas tediosas, extrair insights valiosos de grandes volumes de dados n√£o estruturados e melhorar a intera√ß√£o entre humanos e m√°quinas. Ele desempenha um papel fundamental em muitos aspectos da nossa vida cotidiana, desde pesquisas na web at√© assistentes pessoais em nossos dispositivos m√≥veis.

üöÄ Ao longo desta jornada, para ilustrar o processo da compreens√£o dos computadores acerca dos textos, irei utilizar de um dataframe contra√≠do no Kaggle. O objetivo ser√° criar um modelo de Machine Learning que apresente uma boa acur√°cia na classifica√ß√£o de uma resenha em rela√ß√£o a sentimentos positvos ou negativos.

### Importando as bibliotecas que ser√£o necess√°rias ao estudo: üèõÔ∏è
"""

import nltk
import string
import unicodedata
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go

from nltk import ngrams
from nltk import tokenize
from wordcloud import WordCloud
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

"""# Carregando o dataframe e verificando a sua estrutura e propor√ß√£o dos r√≥tulos : üîç"""

df = pd.read_csv("/content/imdb-reviews-pt-br.csv")

df.head()

def verificaDataFrame(df):

  if df.duplicated().sum() != 0 and df.isna().sum() != 0:

    qt_duplicados = df.duplicated().sum()
    qt_nulos = df.isna().sum()

    print("O dataframe apresenta dados duplicados e nulos")
    print(f"{qt_duplicados} dados duplicados. ")
    print(f"{qt_nulos} dados nulos")
    print("Por padr√£o, realizarei a exclus√£o deles, mas a depender de cada cen√°rio \n uma outra abordagem pode ser necess√°ria.")

    df = df.drop_duplicates()
    df = df.dropna()

    return df

  else:

    print("O dataframe n√£o apresenta dados duplicados.")
    print("Aqui est√° o seu formato: \n")
    print(df.shape)

verificaDataFrame(df)

"""- Propor√ß√£o dos dados: ‚öñÔ∏è

Essa etapa √© importante, pois caso os dados estejam desbalanceados, a an√°lise do modelo de machine learning pode ser interferido, dando a ele vi√©s em suas previs√µes. Nesse cen√°rio em que h√° o desbalanceamento, deve-se adotar uma outra abordagem, segundo a qual fomente uma proporcionalidade dos dados da vari√°vel target.
"""

# Calcular a contagem de valores da coluna 'sentiment'
contagem_sentimentos = df['sentiment'].value_counts()

# Definir os dados para o gr√°fico de pizza
dados_grafico = go.Pie(labels=contagem_sentimentos.index,
                       values=contagem_sentimentos.values,
                       hoverinfo='label+percent',
                       textinfo='percent',
                       textfont=dict(size=20),
                       hole=0.3,
                       pull=[0.1, 0])

# Definir o layout do gr√°fico
layout = go.Layout(title='Distribui√ß√£o de Sentimentos',
                   margin=dict(l=0, r=0, t=30, b=0),
                   title_y=0.9)

# Criar a figura do gr√°fico de pizza
figura = go.Figure(data=[dados_grafico], layout=layout)

# Exibir o gr√°fico
figura.show()

"""### Remodelando o dataframe:

Criando uma coluna, que carregar√° a mesma sem√¢ntica da coluna "sentimento", por√©m em termos num√©ricos, para se aproximar do que √© utilizado segundo o Estado da Arte.
"""

df["classificacao"] = df["sentiment"].replace(["neg", "pos"], [0,1])
df.head()

"""- Analisando brevemente duas resenhas para verificar como s√£o: üî¨"""

df.text_pt[0]

df.text_pt[2]

"""### Passando a entrada de texto ao modelo: ‚û°Ô∏è

Certo, eu possuo o dataframe e o texto da resenha em portugu√™s parece estar consistente. Desse modo, como posso pass√°-lo efetivamente ao modelo ? Basta segmentar numa por√ß√£o de *treino* e *teste* e instanciar um modelo  de machine learning ? N√£o, antes de realizar esse processo, deve-se vetorizar o texto, que em termos pr√°ticos significa extrair cada palavra do texto, passando posteriormente ao processo de "vocabulariza√ß√£o", formando uma bag of words, que informa a quantidade de vezes que uma palavra est√° presente ao texto. Vejamos :

- Vetoriza√ß√£o :
"""

texto = ["Assisti um filme √≥timo", "Assisti a um filme ruim",
         "Assisti a um filme muito muito bom", "Assisti a um filme paia"]

vetorizar = CountVectorizer()

vetorizar.fit_transform(texto)

vetorizar.get_feature_names_out()

"""- "Vocabulariza√ß√£o" :"""

bag_of_words = vetorizar.fit_transform(texto)

matriz_esparsa = pd.DataFrame.sparse.from_spmatrix(bag_of_words,
                                        columns = vetorizar.get_feature_names_out())

matriz_esparsa

"""No dataframe acima, √© poss√≠vel perceber o que havia comentado. Tomando como a lista texto anteriormente passada, nota-se que a quantidade de vezes em que cada palavra esteve presente numa frase.

- OBS‚ùóÔ∏è

Percebeu que o nome da vari√°vel √© matriz esparsa ? Isso n√£o ocorre a toa. No processo de "vocabulariza√ß√£o", que √© perpassado pela vetoriza√ß√£o do texto, palavras ou termos, no sentido mais gen√©rico, que n√£o est√£o presentes na frase recebem um valor nulo, que o algor√≠timo atribui como zero. O nome de uma matriz na qual apresenta alguns elementos diferentes de zero, mas muitos zeros √© justamente matriz esparsa.

### Retornando ao dataframe: üîô
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# bag_of_words = vetorizar.fit_transform(df.text_pt)
# bag_of_words.shape

"""‚ö†Ô∏è

Segundo a sa√≠da acima, nota-se que temos uma matriz de alta dimensionalidade, em especial a quantidade de colunas, que √© de 129.6k aproximadamente. Isso acarreta numa elevada exig√™ncia de processamento, que pode tornar o processo computacional oneroso.

Desse modo, recomenda-se a diminui√ß√£o de features (no que tange as colunas), as quais, apesar de prover uma matriz com menor dimens√£o, conseguem ser eficazes √† an√°lise. Estarei utilizando para esse presente estudo uma max_feature de 50.

- Baseline üìè

> Modelo que representa a baseline do projeto, o ponto inicial, por meio do qual, ao final e ao decorrer do processo poderemos comparar o ganho e/ou perda relativa de acur√°cia, se ocorrer.
"""

def classificar_texto(df, texto, classificacao):

    vetorizar = CountVectorizer(max_features=50)

    bag_of_words = vetorizar.fit_transform(df[texto])

    # Segmentando os dados na sua por√ß√£o treino e teste, da parte do dataframe
    # que me √© pertinente, sendo a coluna em portugu√™s. Em seguida, o outro par√¢metro,
    # passo a propor√ß√£o que deve ser seguida, que √© a vari√°vel target do df.

    train, test, X_train, y_test = train_test_split(bag_of_words,
                                                df.classificacao,
                                                random_state = 22)

    # Modelo de regress√£o log√≠stica para prever um texto de teste
    # a que r√≥tulo ele pertence: positivo ou negativo.

    regressao_logistica = LogisticRegression()
    regressao_logistica.fit(train, X_train)

    # Mensurando a acur√°cia do modelo:
    acuracia = round(regressao_logistica.score(test, y_test), 3)

    print(f"O modelo de baseline apresenta uma acur√°cia aproximada de {acuracia*100} %")
    print("")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# classificar_texto(df, "text_pt", "classificacao")

"""# Tratamento üè•     

Ok, eu j√° tenho o dataframe e um modelo de baseline. H√° algo a mais que possa ser feito ou √© simplesmente isso o PLN ? N√£o, h√° algumas coisas, pelo menos nessa introdu√ß√£o, que podem ser feitas, as quais ir√£o se debru√ßar tanto pelo **tratamento do texto** em si, quanto daquilo que servir√° de **entrada** ao modelo de regress√£o log√≠stica.

Na etapa de tratamento do texto, temos que visar um meio com o qual a mensagem possa ser o t√£o mais eficiente poss√≠vel, sem perder seu conte√∫do sem√¢ntico, de modo que torne o modelo habilitado a generalizar e a identificar padr√µes, contribuindo positivamente com a sua an√°lise. Esquematicamente para essa etapa, tem-se:
- tokeniza√ß√£o;
- remo√ß√£o das stopword, pontua√ß√£o e acentua√ß√£o;
- processo de stemming (encontrar o radical das palavras)  
>

Por outro lado, na esfera da entrada dos dados do modelo, h√° justamente o tratamento desses, que incide sobre os pesos dados √†s palavras que comp√µem o texto (TF-IDF) e sua sequ√™ncia l√≥gica (N-Gramns).

- Verificando as palavras mais frequentes do dataframe üî¨

Mas antes de seguir √†s etapas de tratamento, cabe uma r√°pida visualiza√ß√£o acerca das palavras que s√£o mais frequentes no dataframe.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Nuvem de palavras:
# 
# %%time
# 
# from wordcloud import WordCloud
# 
# # Criando um conjunto que tenha todas as palavras de todas as resenhas do
# # dataframe, como forma de visualizar a preval√™ncia de cada palavra.
# todas_palavras = ' '.join([texto for texto in df.text_pt])
# 
# # Gerando a nuvem de palavras:
# nuvem_palavras = WordCloud(width = 800, height=500,
#                            max_font_size=110,
#                            collocations=False).generate(todas_palavras)
# 
# # OBS:
# # O par√¢metro collocations = False, indica que eu n√£o quero ter
# # na nuvem de palavras bigramas, isto √©, par de palavras, como "um filme",
# # mas apenas uma √∫nica palavra, como "filme", "um", "bom", "ruim" e etc.
# 
# plt.figure(figsize=(10,7))
# plt.imshow(nuvem_palavras, interpolation="bilinear")
# plt.axis('off')  # Remover os n√∫meros do eixo x e y
# plt.show()

"""- Criando um novo dataframe para compreender melhor a frequ√™ncia das palavras."""

# Agora que, formalmente, concebe-se, em s√≠ntese, o que √© um token
# e o processo de tokeniza√ß√£o, podemos tokenizar o nosso texto, al√©m
# do processo de outrora, que foi atrelado ao objetivo de excluir as
# stopwords, que pouco contribuem com a compreens√£o sem√¢ntica do texto.

# Nesse sentido, vamos tokenizar a resenha e depois visualizar em um
# dataframe as palavras que s√£o mais frequentes.

token_space = tokenize.WhitespaceTokenizer()
token_frase = token_space.tokenize(todas_palavras)
frequencia = nltk.FreqDist(token_frase)

df_frequencia = pd.DataFrame({"Palavra": list(frequencia.keys()),
                                   "Frequ√™ncia": list(frequencia.values())})

df_frequencia.head(20)

# Visualizando os itens da tabela via gr√°fico de Pareto:

def pareto(df, coluna ,quantidade):

  plt.figure(figsize=(12,8))

  ax = sns.barplot(data=df.nlargest(columns=coluna, n = quantidade),
                   x = "Palavra", y = "Frequ√™ncia", color = "gray")

  ax.set(ylabel = "Contagem")

  # Rotaciona em 90¬∞ os r√≥tulos do eixo de x.
  plt.xticks(rotation=90)
  plt.show()

pareto(df_frequencia, "Frequ√™ncia", 20)

"""Por meio desse conjunto de informa√ß√µes visuais tornou poss√≠vel identificar a preval√™ncia de termos que pouco dizem respeito a sem√¢ntica do texto em rela√ß√£o a outros ? Nota-se muitos artigos, algumas preposi√ß√µes e etc. Al√©m disso, √© poss√≠vel de perceber palavras com acentua√ß√£o.

A esses termos, desconsiderando, em ess√™ncia, o √∫ltimo, damos o nome de stopwords, os quais precisam ser exclu√≠dos do modelo, para que apenas termos que apresentam relev√¢ncia sem√¢ntica sejam mantidos.

Dado que n√£o apenas os stopwords, mas outros elementos, como ancetua√ß√£o, pontua√ß√£o tamb√©m devem ser exclu√≠dos, a primeira etapa do tratamento do texto se dar√° por excluir tais termos.

### Tratando o texto üìú
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Baixar as stopwords em portugu√™s
# nltk.download('stopwords')
# nltk.download('punkt')
# 
# # Definir as stopwords em portugu√™s
# stopwords_pt = set(stopwords.words('portuguese'))
# 
# # Fun√ß√£o para remover stopwords, pontua√ß√µes e acentua√ß√µes
# def remover_stopwords_pontuacoes_acentos(texto):
#     # Tokenizar o texto
#     tokens = word_tokenize(texto.lower(), language='portuguese')
# 
#     # Remover stopwords, pontua√ß√µes e acentua√ß√µes
#     tokens_sem_stopwords_pontuacoes = [token for token in tokens if token not in stopwords_pt and token.isalpha()]
#     tokens_sem_acentos = [unicodedata.normalize('NFKD', token).encode('ascii', 'ignore').decode('utf-8') for token in tokens_sem_stopwords_pontuacoes]
# 
#     # Reconstruir o texto sem stopwords, pontua√ß√µes e acentua√ß√µes
#     texto_sem_stopwords_pontuacoes_acentos = ' '.join(tokens_sem_acentos)
# 
#     return texto_sem_stopwords_pontuacoes_acentos

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Aplicar a fun√ß√£o ao dataframe e criar uma nova coluna com o texto processado.
# df['tratamento_1'] = df['text_pt'].apply(remover_stopwords_pontuacoes_acentos)
# 
# # Exibir as primeiras linhas do dataframe com a nova coluna:
# df.head()

"""- Verificando brevemente os textos da nova coluna:

Observe que em ambas as resenhas (que s√£o as mesmas anteriormente mostradas) n√£o h√° mais a presen√ßa de acentua√ß√£o, pontua√ß√µes e stopwords.
"""

df.tratamento_1[0]

df.tratamento_1[2]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Verificando novamente o gr√°fico de Pareto,
# # para visualizar as palavras mais frequentes.
# 
# # Criando um conjunto que tenha todas as palavras de todas as resenhas do
# # dataframe, como forma de visualizar a preval√™ncia de cada palavra.
# todas_palavras = ' '.join([texto for texto in df.tratamento_1])
# 
# token_space = tokenize.WhitespaceTokenizer()
# token_frase = token_space.tokenize(todas_palavras)
# frequencia = nltk.FreqDist(token_frase)
# 
# df_frequencia = pd.DataFrame({"Palavra": list(frequencia.keys()),
#                                    "Frequ√™ncia": list(frequencia.values())})

pareto(df_frequencia, "Frequ√™ncia", 20)

"""üó®Ô∏è Observando a imagem acima, nota-se que as palavras com acentua√ß√£o foram exclu√≠das, bem como os acentos e as stopwords, mas ainda h√° algo: note quais s√£o as palavras com maior frequ√™ncia - filme e filmes.

Isso ocorre, no presente notebook, por √≥bvio, tendo em vista que se trata de uma an√°lise dos sentimentos de resenhas de filmes obtidas via um dataframe do kaggle.

Mas o qu√£o √∫til √© permitir que palavras t√£o semelhantes, que, em s√≠ntese, possuem ess√™ncia an√°loga, divergindo apenas a sua quantidade, presentes no modelo ? N√£o seria mais eficiente, por√©m, conservar apenas a sua unidade b√°sica, a sua por√ß√£o radical ? Sim, e esses s√£o os stemms.

### Stemmiza√ß√£o üå±

Stemmiza√ß√£o √© o nome que dei ao processo de encontrar a radical das palavras, desconsiderando o seu tamanho, ou numa linguagem gramatical, seu n√∫mero. Para visualizar, basta pegarmos duas palavras que em ess√™ncia se referem a um mesmo objeto, divergindo apenas em sua quantidade: filme e filmes. Note que o radical de ambas, √© filme.

Mas h√° uma outra forma de eu entender sobre esse processo ? Sim, esse processo busca o radical das palavras, aquilo que nelas √© o mais elementar e as qualificam como tais, ou seja, a sua unidade b√°sica. O efeito disso √© n√£o apenas remover o numeral das palavras, mas tamb√©m seu sufixo e/ou prefixo.

A raz√£o de existir desse processo est√° em reduzir a dimensionalidade dos dados, simplificando o texto a partir do agrupamento de termos semelhantes com base em seus radicais. Desse modo, varia√ß√µes verbais ou de outra natureza n√£o prejudica o modelo.

Por mais contradit√≥rio que possa parecer √† primeira vista, esse processo n√£o interefere negativamente no modelo, mas pode at√© melhor√°-lo, de modo que o torna h√°bil a generalizar melhor e encontrar facilmente mais padr√µes no texto.

- Em s√≠ntese:

O processo de "stemmiza√ß√£o" √© cortar da palavra tudo aquilo que n√£o √© o seu radical, a sua unidade b√°sica, de modo que n√£o interfira negativamente no modelo, mas que, por meio do agrupamento de termos semelhantes, permite a esse a capacidade de generaliza√ß√£o e identifica√ß√£o de padr√µes.
"""

nltk.download('rslp')
nltk.download('punkt')

stemmer = nltk.RSLPStemmer()

print("Exemplo com a palavra corredor: ", stemmer.stem("corredor"), "\n")
print("Exemplo com a palavra correr: ", stemmer.stem("correr"))

# Aplicando o stemm no dataframe:

# Inicializar o stemmer
stemmer = PorterStemmer()

# Fun√ß√£o para aplicar stemming a uma frase
def aplicar_stemming(frase):
    nova_frase = []
    for palavra in word_tokenize(frase):
        stem = stemmer.stem(palavra)
        nova_frase.append(stem)
    return ' '.join(nova_frase)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Aplicar stemming √† coluna 'tratamento_3' do dataframe
# df["tratamento_2"] = df["tratamento_1"].apply(aplicar_stemming)
# 
# # Exibir o dataframe atualizado
# df.head()

# Reformulando o dataframe, para ficar mais elegante:

df = df[["id", "text_en", "text_pt", "tratamento_1", "tratamento_2",
         "sentiment", "classificacao"]]

df.head()

"""### Dividindo a WordCloud em nuvem de sentimentos negativos üëé e positivos üëç.

Verificando a nuvem de palavras ap√≥s o tratamento do texto, verificando a forma delas, visualizando se ainda guardam a sua forma anterior ou n√£o.
"""

def nuvem_palavras_neg(df, coluna):
    texto_negativo = df.query("sentiment == 'neg'")
    todas_palavras = ' '.join([df for df in texto_negativo[coluna]])

    nuvem_palavras = WordCloud(width= 800, height= 500,
                              max_font_size = 110,
                              collocations = False).generate(todas_palavras)
    plt.figure(figsize=(10,7))
    plt.imshow(nuvem_palavras, interpolation='bilinear')
    plt.axis("off")
    plt.show()

def nuvem_palavras_pos(df, coluna):
    texto_negativo = df.query("sentiment == 'pos'")
    todas_palavras = ' '.join([df for df in texto_negativo[coluna]])

    nuvem_palavras = WordCloud(width= 800, height= 500,
                              max_font_size = 110,
                              collocations = False).generate(todas_palavras)
    plt.figure(figsize=(10,7))
    plt.imshow(nuvem_palavras, interpolation='bilinear')
    plt.axis("off")
    plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# nuvem_palavras_neg(df, "tratamento_2")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# nuvem_palavras_pos(df, "tratamento_2")

"""Verificando agora a acur√°cia do modelo ap√≥s o tratamento dos textos."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# print(classificar_texto(df, "tratamento_2", "classificacao"))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# print(classificar_texto(df, "text_pt", "classificacao"))

"""Note que a partir do tratamento do texto, h√° um ganho de um em rela√ß√£o ao outro de 2.3%.

### Tratando a entrada no modelo ü§ñ

- Abordagem TF-IDF üïµÔ∏è

Ok, eu j√° realizei o tratamento do texto de variadas formas, perpassando para a sua padroniza√ß√£o em min√∫scula, removendo artigos e preposi√ß√µes, pontua√ß√µes, bem como reduzindo cada termo em sua unidade b√°sica - ou radical. Mas h√° outra coisa que eu possa fazer para auxiliar o modelo no processo de classifica√ß√£o do sentimento prevalente no texto, com base em sua sem√¢ntica ?

Sim, mas para observar isso, precisamos de um exemplo:
"""

# Definir os dados
texto = ["Assisti um filme √≥timo", "Assisti a um filme ruim",
         "Assisti a um filme muito muito bom", "Assisti a um filme paia"]

# Inicializar o vetorizador
vetorizador = CountVectorizer()

# Vetorizar o texto
bag_of_words = vetorizador.fit_transform(texto)

# Criar um dataframe com as palavras como colunas
df_bw = pd.DataFrame(bag_of_words.toarray(), columns=vetorizador.get_feature_names_out())

# Exibir o dataframe
df_bw

"""üó®Ô∏è Tanto pela lista quanto pela tabela acima, verifica-se que h√° termos que se repetem, na forma que a repeti√ß√£o √© percebida explicitadamente na primeira e compreendida pela quantidade de vezes em que um mesmo termo aparece em linhas diferentes, as quais representam as frases. Com isso uma pergunta pode surgir?

Termos que se repetem em mais de uma das vezes..., enquanto outros, que podem trazer at√© mais relev√¢ncia sem√¢ntica, como os "√≥timos", "bons", "ruins", "p√©ssimos" e etc, aparecem pouco... Isso pode interfirir negativamente no modelo?

Quero dizer, dado que um termo aparece mais, esse pode ter mais impacto na classifica√ß√£o do que outros que aparecem menos ? E se sim, por√©m al√©m: e se esses que aparecem menos podem prover maior car√°ter sem√¢ntico ao modelo ? O que fazer ?

Com base nessa problem√°tica que se utiliza a abordagem do **TF-IDF**, basicamente ela atribui um valor inversamente proporcional √† quantidade de termos que aparecem num conjunto de dados, ou dataset. Nesse sentido, se um termo aparece muitas vezes, esse possuir√° um peso menor, de modo que um outro, que se apresenta numa menor quantidade ter√° um peso maior.

Assim, relacionando com o tema desse notebook, a palavra "film", radical de "filme" ou "filmes", ter√° um peso menor, quando comparada √†s palavras "bom", "√≥timo", "ruim", "p√©ssimo" e etc, que aparecem menos.

- Exempleficando a abordagem üí°
"""

frases = ["Assisti um filme √≥timo", "Assisti um filme p√©ssimo"]

tfidf = TfidfVectorizer(lowercase=False, max_features=50)

caracteristicas = tfidf.fit_transform(frases)

pd.DataFrame(
    caracteristicas.todense(),
    columns=tfidf.get_feature_names_out()
)

"""Por meio da tabela a seguir consegue observar como se d√° o processo de atribui√ß√£o dos pesos? Note que justamente nas palavras que aparecem uma √∫nica vez s√£o aquelas que possuem pesos maiores, de modo que ocorre o oposto naquelas que apresentam uma apari√ß√£o dupla.

- Verificando a acur√°cia:

Agora que j√° entendemos como funciona essa abordagem, segmentaremos as resenhas, compreendendo a resenha em pt-br original e aquela que, a partir dessa, criamos ap√≥s os processos de tratamento, com o objetivo de verificar a acur√°cia, compar√°-la.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # TF-IDF para a coluna n√£o tratada:
# 
# tfidf_bruto = tfidf.fit_transform(df["text_pt"])
# 
# train, test, X_train, y_test = train_test_split(tfidf_bruto,
#                                                 df.classificacao,
#                                                 random_state = 22)
# 
# # Modelo de regress√£o log√≠stica para prever um texto de teste
# # a que r√≥tulo ele pertence: positivo ou negativo.
# 
# regressao_logistica = LogisticRegression()
# regressao_logistica.fit(train, X_train)
# 
# acuracia_tfidf_bruto = regressao_logistica.score(test, y_test)
# 
# # -------------------------------------------------------------------------------- #
# 
# # TF-IDF para a coluna tratada e stemmizada:
# 
# tfidf_tratados = tfidf.fit_transform(df["tratamento_2"])
# 
# train, test, X_train, y_test = train_test_split(tfidf_tratados,
#                                                 df.classificacao,
#                                                 random_state = 22)
# 
# regressao_logistica.fit(train, X_train)
# acuracia_tfidf_tratados = regressao_logistica.score(test, y_test)
# 
# 
# print(f"A acur√°cia da coluna sem tratamento, utilizando o TF-IDF √© de {round(acuracia_tfidf_bruto ,3)*100} %")
# print("")
# print(f"A acur√°cia da coluna com tratamento e stemmizada, utilizando o TF-IDF √© de {round(acuracia_tfidf_tratados ,3)*100} %")
# print("")

"""Observando as novas acur√°cias com base na nova abordagem utilizada, nota-se que o modelo apresentou um ganho de 0.2 %. Ou seja, apresenta um ganho, por√©m ainda n√£o t√£o significativo.

### N-Gramns: üìö

Para passar um texto em um modelo, precisei vetoriz√°-lo, al√©m de tokeniz√°-lo por meio das "n" etapas de tratamentos necess√°rias, visando a sua consist√™ncia e capacidade de generaliza√ß√£o e identifica√ß√£o de padr√µes, certo? Por√©m, veja, utilizar a vetoriza√ß√£o normal, criando a bag_of_words, composta de palavra por palavra, n√£o pode ser necessariamente o mais eficiente.

Por isso surge a abordagem dos n-gramns, recurso que permite representar sequ√™ncias de palavras em um texto de uma forma estruturada e contextualizada. Em termos simples, um N-grama √© uma sequ√™ncia cont√≠nua de N itens consecutivos de uma amostra de texto. Esses itens podem ser caracteres, palavras, ou at√© mesmo s√≠mbolos, dependendo do contexto em que est√£o sendo utilizados.

Por exemplo, considere a frase: "Eu gosto de sorvete de chocolate". Se decidirmos dividir essa frase em bi-gramas (2-grams), obteremos:

- "Eu gosto"
- "gosto de"
- "de sorvete"
- "sorvete de"
- "de chocolate"

üí¨ Agora, se optarmos por dividir a mesma frase em tri-gramas (3-grams), obteremos:

- "Eu gosto de"
- "gosto de sorvete"
- "de sorvete de"
- "sorvete de chocolate"

üîç Os N-grams s√£o √∫teis porque capturam a estrutura e o contexto do texto de uma forma mais detalhada do que palavras individuais. Ao analisar os N-grams em um texto, podemos entender melhor como as palavras est√£o relacionadas umas com as outras e como elas contribuem para o significado global da frase ou do documento.

- Verificando os N-gramns em c√≥digo:
"""

from nltk import ngrams

frase = "Assisti um √≥timo filme."
frase_separada = token_space.tokenize(frase)

print("Como ficava a frase sem utilizar o ngrams: \n", frase_separada)
print("")

pares = ngrams(frase_separada, 2)
print("A frase em pares: \n", list(pares))

print("")

trio = ngrams(frase_separada, 3)
print("A frase em trio: \n", list(trio))

"""üí¨ Conseguiu observar a diferen√ßa ? Observou como o ngrams garante uma sequ√™ncia por meio do agrupamento que forma, promovendo uma sequ√™ncia, como esp√©cime de mem√≥ria, a qual pode contribuir com a an√°lise sem√¢ntica do texto, diferente do primeiro processo em que n√£o h√° a utiliza√ß√£o do ngrams?

Nesse sentido, al√©m dos processos de tratamento do texto, perpassando pela remo√ß√£o da pontua√ß√£o, dos stopwords, da stemmiza√ß√£o e do TF-IDF, recomenda-se para a an√°lise sem√¢ntica do modelo de PLN, que os textos estejam segundo os N-gramns, que garantem uma sequ√™ncia l√≥gica das frases, contribuindo com a an√°lise da sem√¢ntica to texto.

- Utilizando os N-gramns, ao modelo e verificar se h√° ou n√£o ganho em sua acur√°cia:
"""

from tqdm import tqdm_notebook

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Inicialize o TfidfVectorizer
# tfidf = TfidfVectorizer(ngram_range=(1, 2))
# 
# # Ajuste o TfidfVectorizer e obtenha a representa√ß√£o TF-IDF dos dados
# vetor_tfidf = tfidf.fit_transform(df["tratamento_2"])
# 
# # Crie uma barra de progresso para monitorar o progresso
# progresso = tqdm_notebook(total=vetor_tfidf.shape[0], desc="Criando vetor TF-IDF")
# 
# # Segmenta√ß√£o:
# treino, teste, classe_treino, classe_teste = train_test_split(vetor_tfidf,
#                                                               df["classificacao"],
#                                                               random_state=22)
# 
# # Modelo:
# regressao_logistica = LogisticRegression()
# regressao_logistica.fit(treino, classe_treino)
# 
# # Atualize a barra de progresso
# progresso.update(vetor_tfidf.shape[0])
# progresso.close()
# 
# # Calcule a acur√°cia do modelo
# acuracia_tfidf_ngrams = regressao_logistica.score(teste, classe_teste)
# 
# # Exiba a acur√°cia
# print(f"A acur√°cia do modelo, com a utiliza√ß√£o dos n-gramns foi de {round(acuracia_tfidf_ngrams, 3)*100} %")
# print("")

"""Note como a utiliza√ß√£o dos N-gramns foi eficiente ao modelo. Antes ele possuia uma acur√°cia de 69.5%, agora ele possui uma acur√°cia de 89.2%, representando um salto de pouco mais que 20% de um em rela√ß√£o ao outro.

### Vericando quais s√£o as vari√°veis explicativas que contribuem com a resposta do modelo: üîç

Algo que o modelo de regress√£o log√≠stica permite realizar √© a verifica√ß√£o do quanto certas vari√°veis contribuem ou n√£o a resposta do modelo. Nesse sentido, no presente contexto, podemos analisar quais s√£o as vari√°veis que impactam tanto numa classifica√ß√£o de sentimento tida como positiva quanto de negativa, vejamos:

- Sentimento : "pos"
"""

pesos = pd.DataFrame(
    regressao_logistica.coef_[0].T,
    index = tfidf.get_feature_names_out()
)
pesos.nlargest(10, 0)

"""- Sentimento: "neg"
"""

pesos.nsmallest(10, 0)

